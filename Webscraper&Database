import requests
from bs4 import BeautifulSoup
import sqlite3
import os
from urllib.parse import urljoin

response = requests.get('https://httpbin.org/user-agent')
user_agent = response.json()['user-agent']

def getScholarData(url, user_agent=user_agent):
    try:
        headers = {
            "User-Agent": user_agent
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        scholar_results = []

        for el in soup.select(".gs_r")[:3]:  # Limit to top 3 results
            title_link = el.select(".gs_rt a")[0]["href"] if el.select(".gs_rt a") else ""
            # Only process links that are not PDFs
            if not title_link.lower().endswith('.pdf'):
                scholar_results.append({
                    "title": el.select(".gs_rt")[0].text if el.select(".gs_rt") else None,
                    "title_link": title_link,
                    "id": el.select(".gs_rt a")[0]["id"] if el.select(".gs_rt a") else "",
                    "displayed_link": el.select(".gs_a")[0].text if el.select(".gs_a") else "",
                    "snippet": el.select(".gs_rs")[0].text.replace("\n", "") if el.select(".gs_rs") else "",
                    "cited_by_count": el.select(".gs_nph+ a")[0].text if el.select(".gs_nph+ a") else "",
                    "cited_link": "https://scholar.google.com" + el.select(".gs_nph+ a")[0]["href"] if el.select(".gs_nph+ a") and len(el.select(".gs_nph+ a")) > 0 else None,
                    "versions_count": el.select("a~ a+ .gs_nph")[0].text if el.select("a~ a+ .gs_nph") and len(el.select("a~ a+ .gs_nph")) > 0 else "",
                    "versions_link": "https://scholar.google.com" + el.select("a~ a+ .gs_nph")[0]["href"] if el.select("a~ a+ .gs_nph") and len(el.select("a~ a+ .gs_nph")) > 0 and el.select("a~ a+ .gs_nph")[0].text else "",
                })
        # Filter out items with empty or None values
        for i in range(len(scholar_results)):
            scholar_results[i] = {key: value for key, value in scholar_results[i].items() if value != "" and value is not None}

        return scholar_results
    except requests.exceptions.RequestException as e:
        print(f"Request error: {e}")
        return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []


def init_db():
    conn = sqlite3.connect('scholar_data.db')
    c = conn.cursor()

    # Drop the table if it exists
    c.execute("DROP TABLE IF EXISTS scholar_results")

    # Recreate the table with the correct schema
    c.execute('''
        CREATE TABLE IF NOT EXISTS scholar_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            topic TEXT,
            title TEXT,
            website_link TEXT,
            displayed_link TEXT,
            snippet TEXT,
            cited_by_count TEXT,
            cited_link TEXT,
            versions_count TEXT,
            versions_link TEXT
        )
    ''')
    conn.commit()
    conn.close()


def insert_data(topic, title, website_link, displayed_link, snippet, cited_by_count, cited_link, versions_count, versions_link):
    conn = sqlite3.connect('scholar_data.db')
    c = conn.cursor()
    c.execute('''
        INSERT INTO scholar_results (topic, title, website_link, displayed_link, snippet, cited_by_count, cited_link, versions_count, versions_link)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (topic, title, website_link, displayed_link, snippet, cited_by_count, cited_link, versions_count, versions_link))
    conn.commit()
    conn.close()

def download_and_store_websites(topics):
    """Main function to download, process, and store website links"""
    base_url = "https://www.google.com/scholar?q={}&hl=en"
    for topic in topics:
        url = base_url.format(topic.replace(" ", "+"))
        results = getScholarData(url)
        for result in results:
            # Get the website URL from the search result
            website_url = result['title_link']
            website_url = urljoin("https://scholar.google.com", website_url)  # Ensure the URL is absolute
            
            # Save the website link and title in the database (excluding PDFs)
            insert_data(topic, result['title'], website_url, result['displayed_link'], result['snippet'], result['cited_by_count'], result['cited_link'], result['versions_count'], result['versions_link'])

init_db()
topics = ["Medical Devices", "Artificial Intelligence"]
download_and_store_websites(topics)
